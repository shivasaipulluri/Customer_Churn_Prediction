{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bcc9bb-7cc8-4db6-86e1-6f5ee10ae761",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/features_with_interactions.csv\")  # Updated features dataset with interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78116b09-64e5-46bf-b850-b15a1987bf3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m mlp_model(batch_X)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# ✅ Fix Shape Mismatch\u001b[39;00m\n\u001b[0;32m     77\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# ✅ Ensure same dtype & shape\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# ✅ Correct Loss Computation\u001b[39;00m\n\u001b[0;32m     79\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     80\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "# 📌 Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 📌 Step 2: Load Dataset with Feature Interactions\n",
    "df = pd.read_csv(\"../data/features_with_interactions.csv\")  # Updated features dataset with interactions\n",
    "y = pd.read_csv(\"../data/hidden_churn_labels.csv\")[\"Churn\"]  # Load hidden churn labels\n",
    "\n",
    "# 📌 Step 3: Identify Categorical & Numerical Features\n",
    "categorical_features = ['Gender', 'Income_Group', 'Customer_Type', 'Residency_Status', 'Account_Type', 'Credit_Score']\n",
    "numerical_features = [col for col in df.columns if col not in categorical_features and col != \"Customer_ID\"]\n",
    "\n",
    "# 📌 Step 4: Data Preprocessing Pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "X_transformed = preprocessor.fit_transform(df.drop(columns=[\"Customer_ID\"]))\n",
    "feature_names = numerical_features + list(preprocessor.transformers_[1][1].get_feature_names_out(categorical_features))\n",
    "\n",
    "# 📌 Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).view(-1, 1)  # ✅ Fix Shape Mismatch\n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).view(-1, 1)  # ✅ Fix Shape Mismatch\n",
    "\n",
    "# 📌 Step 6: Define MLP Model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # ✅ Ensures output is between [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "mlp_model = MLPClassifier(input_size)\n",
    "\n",
    "# 📌 Step 7: Define Loss & Optimizer\n",
    "criterion = nn.BCELoss()  # ✅ Binary Cross-Entropy Loss for Classification\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
    "\n",
    "# 📌 Step 8: Train the MLP Model\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_X).view(-1, 1)  # ✅ Fix Shape Mismatch\n",
    "        batch_y = batch_y.view(-1, 1)  # ✅ Ensure same dtype & shape\n",
    "        loss = criterion(outputs, batch_y)  # ✅ Correct Loss Computation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# 📌 Step 9: Evaluate the Model\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = mlp_model(X_test_tensor).view(-1).numpy()  # ✅ Fix Shape Issue\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute Performance Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"📊 Model Performance (MLP):\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✅ Precision: {precision:.4f}\")\n",
    "print(f\"✅ Recall: {recall:.4f}\")\n",
    "print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n🔍 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 Step 10: SHAP Values Calculation\n",
    "explainer = shap.Explainer(mlp_model, X_train_tensor)\n",
    "shap_values = explainer(X_test_tensor)\n",
    "\n",
    "# 📌 Step 11: SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values.values, X_test, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c47020-6191-4ccc-98f9-eb314d9f4c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc62447-9bd1-43c3-a725-33f0e2c3a146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4581b8f-c43b-4a50-b54f-801503261d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.5910\n",
      "Epoch [2/20], Loss: 0.5870\n",
      "Epoch [3/20], Loss: 0.5860\n",
      "Epoch [4/20], Loss: 0.5859\n",
      "Epoch [5/20], Loss: 0.5853\n",
      "Epoch [6/20], Loss: 0.5849\n",
      "Epoch [7/20], Loss: 0.5848\n",
      "Epoch [8/20], Loss: 0.5845\n",
      "Epoch [9/20], Loss: 0.5842\n",
      "Epoch [10/20], Loss: 0.5842\n",
      "Epoch [11/20], Loss: 0.5841\n",
      "Epoch [12/20], Loss: 0.5837\n",
      "Epoch [13/20], Loss: 0.5835\n",
      "Epoch [14/20], Loss: 0.5835\n",
      "Epoch [15/20], Loss: 0.5833\n",
      "Epoch [16/20], Loss: 0.5829\n",
      "Epoch [17/20], Loss: 0.5828\n",
      "Epoch [18/20], Loss: 0.5828\n",
      "Epoch [19/20], Loss: 0.5822\n",
      "Epoch [20/20], Loss: 0.5822\n",
      "\n",
      "📊 Model Performance (MLP):\n",
      "✅ Accuracy: 0.7283\n",
      "✅ Precision: 0.0000\n",
      "✅ Recall: 0.0000\n",
      "✅ F1 Score: 0.0000\n",
      "✅ ROC-AUC Score: 0.5000\n",
      "\n",
      "🔍 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84     14566\n",
      "           1       0.00      0.00      0.00      5434\n",
      "\n",
      "    accuracy                           0.73     20000\n",
      "   macro avg       0.36      0.50      0.42     20000\n",
      "weighted avg       0.53      0.73      0.61     20000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pullu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\pullu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\pullu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\pullu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 125\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# 📌 Step 10: SHAP Values Calculation\u001b[39;00m\n\u001b[0;32m    124\u001b[0m explainer \u001b[38;5;241m=\u001b[39m shap\u001b[38;5;241m.\u001b[39mExplainer(mlp_model, X_train_tensor)\n\u001b[1;32m--> 125\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# 📌 Step 11: SHAP Summary Plot\u001b[39;00m\n\u001b[0;32m    128\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_permutation.py:100\u001b[0m, in \u001b[0;36mPermutationExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m     silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     98\u001b[0m ):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Explain the output of the model on the given arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_explainer.py:366\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[0;32m    364\u001b[0m     feature_names \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(args))]\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_args \u001b[38;5;129;01min\u001b[39;00m show_progress(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39margs), num_rows, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m explainer\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent):\n\u001b[1;32m--> 366\u001b[0m     row_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain_row\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrow_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmain_effects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmain_effects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_bounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    376\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    377\u001b[0m     output_indices\u001b[38;5;241m.\u001b[39mappend(row_result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\explainers\\_permutation.py:162\u001b[0m, in \u001b[0;36mPermutationExplainer.explain_row\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[0;32m    159\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# evaluate the masked model\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m row_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     row_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(fm),) \u001b[38;5;241m+\u001b[39m outputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\utils\\_masked_model.py:64\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[1;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[0;32m     62\u001b[0m         full_masks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39msum(masks \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_masker_cols), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     63\u001b[0m         _convert_delta_mask_to_full(masks, full_masks)\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_full_masking_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_masking_call(masks, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\shap\\utils\\_masked_model.py:91\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[1;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[0;32m     89\u001b[0m     masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker(delta_ind, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 91\u001b[0m     masked_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasker\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# get a copy that won't get overwritten by the next iteration\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasker, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimmutable_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "# 📌 Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 📌 Step 2: Load Dataset with Feature Interactions\n",
    "df = pd.read_csv(\"../data/features.csv\")  # Use the correct dataset\n",
    "y = pd.read_csv(\"../data/hidden_churn_labels.csv\")[\"Churn\"]\n",
    "\n",
    "# 📌 Step 3: Identify Categorical & Numerical Features\n",
    "categorical_features = ['Gender', 'Income_Group', 'Customer_Type', 'Residency_Status', 'Account_Type', 'Credit_Score']\n",
    "numerical_features = [col for col in df.columns if col not in categorical_features and col != \"Customer_ID\"]\n",
    "\n",
    "# 📌 Step 4: Data Preprocessing Pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "X_transformed = preprocessor.fit_transform(df.drop(columns=[\"Customer_ID\"]))\n",
    "feature_names = numerical_features + list(preprocessor.transformers_[1][1].get_feature_names_out(categorical_features))\n",
    "\n",
    "# 📌 Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).view(-1, 1)  \n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).view(-1, 1)  \n",
    "\n",
    "# 📌 Fix 1: Handle NaN values in tensor data\n",
    "X_train_tensor = torch.nan_to_num(X_train_tensor, nan=0.0)\n",
    "y_train_tensor = torch.nan_to_num(y_train_tensor, nan=0.0)\n",
    "\n",
    "# 📌 Fix 2: Adjust Learning Rate (Avoid Exploding Gradients)\n",
    "learning_rate = 0.0005  \n",
    "\n",
    "# 📌 Fix 3: Initialize Model Weights Properly\n",
    "def initialize_weights(model):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)  # Xavier Initialization\n",
    "            layer.bias.data.fill_(0.01)\n",
    "\n",
    "# 📌 Step 6: Define MLP Model\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Ensure output values are between [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "mlp_model = MLPClassifier(input_size)\n",
    "initialize_weights(mlp_model)  # Apply weight initialization\n",
    "\n",
    "# 📌 Step 7: Define Loss & Optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 📌 Step 8: Train the MLP Model\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_X).view(-1, 1)  # Ensure shape matches\n",
    "        batch_y = batch_y.view(-1, 1)  # Ensure target shape matches\n",
    "        loss = criterion(outputs, batch_y)  # Compute loss correctly\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print loss for debugging\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# 📌 Step 9: Evaluate the Model\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = mlp_model(X_test_tensor).view(-1).numpy()  # Ensure correct shape\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute Performance Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\n📊 Model Performance (MLP):\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✅ Precision: {precision:.4f}\")\n",
    "print(f\"✅ Recall: {recall:.4f}\")\n",
    "print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "print(\"\\n🔍 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 Step 10: SHAP Values Calculation\n",
    "explainer = shap.Explainer(mlp_model, X_train_tensor)\n",
    "shap_values = explainer(X_test_tensor)\n",
    "\n",
    "# 📌 Step 11: SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values.values, X_test, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc31fc6-78e0-46f2-a5c4-c9c0bd3e4508",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [64, 1] doesn't match the broadcast shape [64, 80000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# ✅ Fix Values Outside [0,1]\u001b[39;00m\n\u001b[0;32m    106\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(outputs, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-7\u001b[39m)  \n\u001b[1;32m--> 108\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# ✅ Gradient Clipping to Avoid Exploding Gradients\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3127\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3124\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3125\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: output with shape [64, 1] doesn't match the broadcast shape [64, 80000]"
     ]
    }
   ],
   "source": [
    "# 📌 Step 1: Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 📌 Step 2: Load Dataset with Feature Interactions\n",
    "df = pd.read_csv(\"../data/features_with_interactions.csv\")  \n",
    "y = pd.read_csv(\"../data/hidden_churn_labels.csv\")[\"Churn\"]\n",
    "\n",
    "# 📌 Step 3: Identify Categorical & Numerical Features\n",
    "categorical_features = ['Gender', 'Income_Group', 'Customer_Type', 'Residency_Status', 'Account_Type', 'Credit_Score']\n",
    "numerical_features = [col for col in df.columns if col not in categorical_features and col != \"Customer_ID\"]\n",
    "\n",
    "# 📌 Step 4: Data Preprocessing Pipeline\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "# Apply transformation\n",
    "X_transformed = preprocessor.fit_transform(df.drop(columns=[\"Customer_ID\"]))\n",
    "feature_names = numerical_features + list(preprocessor.transformers_[1][1].get_feature_names_out(categorical_features))\n",
    "\n",
    "# 📌 Step 5: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train.astype(np.float32))\n",
    "X_test_tensor = torch.tensor(X_test.astype(np.float32))\n",
    "y_train_tensor = torch.tensor(y_train.values.astype(np.float32)).view(-1, 1)  \n",
    "y_test_tensor = torch.tensor(y_test.values.astype(np.float32)).view(-1, 1)  \n",
    "\n",
    "# ✅ Fix NaNs in Training Data\n",
    "X_train_tensor = torch.nan_to_num(X_train_tensor, nan=0.0)\n",
    "y_train_tensor = torch.nan_to_num(y_train_tensor, nan=0.0)\n",
    "\n",
    "# 📌 Step 6: Fix Class Imbalance with Weighted Loss\n",
    "class_counts = y_train.value_counts().to_dict()\n",
    "weight_0 = 1 / class_counts[0]\n",
    "weight_1 = 1 / class_counts[1]\n",
    "weights = torch.tensor([weight_0, weight_1], dtype=torch.float32)\n",
    "\n",
    "# 📌 Step 7: Define MLP Model with Fixes\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.BatchNorm1d(128),  # ✅ Batch Normalization\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()  # ✅ Ensure outputs in [0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "mlp_model = MLPClassifier(input_size)\n",
    "\n",
    "# 📌 Step 8: Weight Initialization\n",
    "def initialize_weights(model):\n",
    "    for layer in model.children():\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            layer.bias.data.fill_(0.01)  # ✅ Avoid dead neurons\n",
    "\n",
    "initialize_weights(mlp_model)\n",
    "\n",
    "# 📌 Step 9: Define Weighted Loss & Optimizer\n",
    "criterion = nn.BCELoss(weight=weights[y_train_tensor.long()].squeeze())  \n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.002)\n",
    "\n",
    "# 📌 Step 10: Train the MLP Model with Learning Rate Decay & Gradient Clipping\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)  # ✅ Learning Rate Decay\n",
    "\n",
    "num_epochs = 25\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp_model(batch_X).view(-1, 1)\n",
    "        batch_y = batch_y.view(-1, 1)\n",
    "\n",
    "        # ✅ Fix Values Outside [0,1]\n",
    "        outputs = torch.clamp(outputs, min=1e-7, max=1 - 1e-7)  \n",
    "\n",
    "        loss = criterion(outputs, batch_y)  \n",
    "        loss.backward()\n",
    "\n",
    "        # ✅ Gradient Clipping to Avoid Exploding Gradients\n",
    "        torch.nn.utils.clip_grad_norm_(mlp_model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    scheduler.step()  # ✅ Update learning rate\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 📌 Step 11: Evaluate the Model\n",
    "with torch.no_grad():\n",
    "    y_pred_proba = mlp_model(X_test_tensor).view(-1).numpy()\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Compute Performance Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(f\"📊 Model Performance (MLP with Fixes):\")\n",
    "print(f\"✅ Accuracy: {accuracy:.4f}\")\n",
    "print(f\"✅ Precision: {precision:.4f}\")\n",
    "print(f\"✅ Recall: {recall:.4f}\")\n",
    "print(f\"✅ F1 Score: {f1:.4f}\")\n",
    "print(f\"✅ ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "print(\"\\n🔍 Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 📌 Step 12: SHAP Values Calculation (Fixed)\n",
    "X_test_numpy = X_test_tensor.numpy()\n",
    "\n",
    "explainer = shap.Explainer(mlp_model, X_train_tensor)\n",
    "shap_values = explainer(X_test_numpy)  \n",
    "\n",
    "# 📌 Step 13: SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values.values, X_test_numpy, feature_names=feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d860ce-63b2-4389-b6c4-e21b32c68a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
